{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment all the cells with !pip to install the dependencies\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimagesearch.transform import four_point_transform\n",
    "from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --upgrade imutils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGE Detection, Finding Contours and Saving the processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please place your raw images under Raw_images folder(for testting there is already an image placed)\n",
    "# Processed images can be found under processed folder\n",
    "#For speeding up the edge detection, image is resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walmart.10.jpg']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "raw_img='Raw_images'\n",
    "onlyfiles = [ f for f in listdir(raw_img) if isfile(join(raw_img,f)) ]\n",
    "print(onlyfiles)\n",
    "processed_img='processed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw_imagesWalmart.10.jpg\n",
      "(3264, 2448, 3)\n",
      "(500, 375, 3)\n",
      "Edge Detection\n",
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n"
     ]
    }
   ],
   "source": [
    "for imglist in onlyfiles:\n",
    "    print(raw_img+imglist)\n",
    "    image = cv2.imread(raw_img+'/'+imglist)\n",
    "    print(image.shape)\n",
    "    #For speeding up the edge detection, image is resized\n",
    "    ratio = image.shape[0] / 500.0 # Ratio of original to new height\n",
    "    orig = image.copy()\n",
    "    image = imutils.resize(image, height = 500)\n",
    "    print(image.shape)\n",
    "    # convert the image to grayscale, blur it, and find edges\n",
    "    # in the image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edged = cv2.Canny(gray, 75, 200)\n",
    "    print(\"Edge Detection\")\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.imshow(\"Edged\", edged)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "         # approximate the contour\n",
    "         peri = cv2.arcLength(c, True)\n",
    "         approx = cv2.approxPolyDP(c, 0.01 * peri, True)\n",
    " \n",
    "         # if our approximated contour has four points, then we\n",
    "         # can assume that we have found our receipt\n",
    "         if len(approx) == 4:\n",
    "                  screenCnt = approx\n",
    "                  break\n",
    "    print(\"STEP 2: Find contours of paper\")\n",
    "    cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Outline\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(processed_img+'/'+'processed'+imglist,warped)\n",
    "    print(\"STEP 3: Apply perspective transform\")\n",
    "    cv2.imshow(\"Original\", imutils.resize(orig, height = 650))\n",
    "    cv2.imshow(\"Scanned\", imutils.resize(warped, height = 650))\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Finding Contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold_local(image, block_size, method='gaussian', offset=0, mode='reflect', param=None, cval=0)[source]\n",
    "Compute a threshold mask image based on local pixel neighborhood.\n",
    "\n",
    "Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Alternatively the threshold can be determined dynamically by a given function, using the ‘generic’ method.\n",
    "\n",
    "Parameters\n",
    "image(N, M) ndarray\n",
    "Input image.\n",
    "\n",
    "block_sizeint\n",
    "Odd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, …, 21, …).\n",
    "\n",
    "method{‘generic’, ‘gaussian’, ‘mean’, ‘median’}, optional\n",
    "Method used to determine adaptive threshold for local neighbourhood in weighted mean image.\n",
    "\n",
    "‘generic’: use custom function (see param parameter)\n",
    "\n",
    "‘gaussian’: apply gaussian filter (see param parameter for custom sigma value)\n",
    "\n",
    "‘mean’: apply arithmetic mean filter\n",
    "\n",
    "‘median’: apply median rank filter\n",
    "\n",
    "By default the ‘gaussian’ method is used.\n",
    "\n",
    "offsetfloat, optional\n",
    "Constant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0.\n",
    "\n",
    "mode{‘reflect’, ‘constant’, ‘nearest’, ‘mirror’, ‘wrap’}, optional\n",
    "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to ‘constant’. Default is ‘reflect’.\n",
    "\n",
    "param{int, function}, optional\n",
    "Either specify sigma for ‘gaussian’ method or function object for ‘generic’ method. This functions takes the flat array of local neighbourhood as a single argument and returns the calculated threshold for the centre pixel.\n",
    "\n",
    "cvalfloat, optional\n",
    "Value to fill past edges of input if mode is ‘constant’.\n",
    "\n",
    "Returns\n",
    "threshold(N, M) ndarray\n",
    "Threshold image. All pixels in the input image higher than the corresponding pixel in the threshold image are considered foreground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropping the images to get the Core area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main function\n",
      "setPath function\n",
      "C:\\Users\\INDRA\\Documents\\DSP-Project\\processed\n",
      "cropPhotos function\n",
      "C:\\Users\\INDRA\\Documents\\DSP-Project\\processed\n"
     ]
    }
   ],
   "source": [
    "# file present in the same directory\n",
    "%run -i \"photocrop.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(scores, geometry):\n",
    "         # grab the number of rows and columns from the scores volume, then\n",
    "         # initialize our set of bounding box rectangles and corresponding\n",
    "         # confidence scores\n",
    "         (numRows, numCols) = scores.shape[2:4]\n",
    "         rects = []\n",
    "         confidences = []\n",
    " \n",
    "         # loop over the number of rows\n",
    "         for y in range(0, numRows):\n",
    "                  # extract the scores (probabilities), followed by the\n",
    "                  # geometrical data used to derive potential bounding box\n",
    "                  # coordinates that surround text\n",
    "                  scoresData = scores[0, 0, y]\n",
    "                  xData0 = geometry[0, 0, y]\n",
    "                  xData1 = geometry[0, 1, y]\n",
    "                  xData2 = geometry[0, 2, y]\n",
    "                  xData3 = geometry[0, 3, y]\n",
    "                  anglesData = geometry[0, 4, y]\n",
    " \n",
    "                  # loop over the number of columns\n",
    "                  for x in range(0, numCols):\n",
    "                           # if our score does not have sufficient probability,\n",
    "                           # ignore it\n",
    "                           if scoresData[x] < 0.01:\n",
    "                                    continue\n",
    " \n",
    "                           # compute the offset factor as our resulting feature\n",
    "                           # maps will be 4x smaller than the input image\n",
    "                           (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    " \n",
    "                           # extract the rotation angle for the prediction and\n",
    "                           # then compute the sin and cosine\n",
    "                           angle = anglesData[x]\n",
    "                           cos = np.cos(angle)\n",
    "                           sin = np.sin(angle)\n",
    " \n",
    "                           # use the geometry volume to derive the width and height\n",
    "                           # of the bounding box\n",
    "                           h = xData0[x] + xData2[x]\n",
    "                           w = xData1[x] + xData3[x]\n",
    " \n",
    "                           # compute both the starting and ending (x, y)-coordinates\n",
    "                           # for the text prediction bounding box\n",
    "                           endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "                           endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "                           startX = int(endX - w)\n",
    "                           startY = int(endY - h)\n",
    " \n",
    "                           # add the bounding box coordinates and probability score\n",
    "                           # to our respective lists\n",
    "                           rects.append((startX, startY, endX, endY))\n",
    "                           confidences.append(scoresData[x])\n",
    " \n",
    "         # return a tuple of the bounding boxes and associated confidences\n",
    "         return (rects, confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(900, 843)\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"processed/1_processedWalmart.10.jpg\")\n",
    "print(type(image))\n",
    "orig = image.copy()\n",
    "(origH, origW) = image.shape[:2]\n",
    "print((origH, origW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new width and height and save the old to new ratio's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "(newW, newH) = (320, 320)\n",
    "rW = origW / float(newW)\n",
    "rH = origH / float(newH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 320, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.resize(image, (newW, newH))\n",
    "(H, W) = image.shape[:2]\n",
    "print(image.shape)\n",
    "cv2.imshow(\"Image\",image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# East text detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerNames = [\n",
    "  \"feature_fusion/Conv_7/Sigmoid\",\n",
    "  \"feature_fusion/concat_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 58.32      59.32      58.32     ...  73.32      73.32\n",
      "     74.32    ]\n",
      "   [ 60.32      58.32      60.32     ...  73.32      73.32\n",
      "     74.32    ]\n",
      "   [ 59.32      61.32      59.32     ...  73.32      72.32\n",
      "     73.32    ]\n",
      "   ...\n",
      "   [ 22.32      17.32      20.32     ...  80.32      77.32\n",
      "     79.32    ]\n",
      "   [ 24.32      19.32      21.32     ...  77.32      78.32\n",
      "     79.32    ]\n",
      "   [ 21.32      20.32      21.32     ...  78.32      79.32\n",
      "     80.32    ]]\n",
      "\n",
      "  [[ 65.22      66.22      65.22     ...  80.22      80.22\n",
      "     81.22    ]\n",
      "   [ 67.22      65.22      67.22     ...  80.22      80.22\n",
      "     81.22    ]\n",
      "   [ 66.22      68.22      66.22     ...  80.22      79.22\n",
      "     80.22    ]\n",
      "   ...\n",
      "   [ 29.220001  24.220001  27.220001 ...  87.22      84.22\n",
      "     86.22    ]\n",
      "   [ 31.220001  26.220001  28.220001 ...  84.22      85.22\n",
      "     86.22    ]\n",
      "   [ 28.220001  27.220001  28.220001 ...  85.22      86.22\n",
      "     87.22    ]]\n",
      "\n",
      "  [[ 78.06      79.06      78.06     ...  93.06      93.06\n",
      "     94.06    ]\n",
      "   [ 80.06      78.06      80.06     ...  93.06      93.06\n",
      "     94.06    ]\n",
      "   [ 79.06      81.06      79.06     ...  93.06      92.06\n",
      "     93.06    ]\n",
      "   ...\n",
      "   [ 42.059998  37.059998  40.059998 ... 100.06      97.06\n",
      "     99.06    ]\n",
      "   [ 44.059998  39.059998  41.059998 ...  97.06      98.06\n",
      "     99.06    ]\n",
      "   [ 41.059998  40.059998  41.059998 ...  98.06      99.06\n",
      "    100.06    ]]]]\n",
      "First Blob: (1, 3, 320, 320)\n"
     ]
    }
   ],
   "source": [
    "blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "print(blob)\n",
    "print(\"First Blob: {}\".format(blob.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 58.32      59.32      58.32     ...  73.32      73.32\n",
      "     74.32    ]\n",
      "   [ 60.32      58.32      60.32     ...  73.32      73.32\n",
      "     74.32    ]\n",
      "   [ 59.32      61.32      59.32     ...  73.32      72.32\n",
      "     73.32    ]\n",
      "   ...\n",
      "   [ 22.32      17.32      20.32     ...  80.32      77.32\n",
      "     79.32    ]\n",
      "   [ 24.32      19.32      21.32     ...  77.32      78.32\n",
      "     79.32    ]\n",
      "   [ 21.32      20.32      21.32     ...  78.32      79.32\n",
      "     80.32    ]]\n",
      "\n",
      "  [[ 65.22      66.22      65.22     ...  80.22      80.22\n",
      "     81.22    ]\n",
      "   [ 67.22      65.22      67.22     ...  80.22      80.22\n",
      "     81.22    ]\n",
      "   [ 66.22      68.22      66.22     ...  80.22      79.22\n",
      "     80.22    ]\n",
      "   ...\n",
      "   [ 29.220001  24.220001  27.220001 ...  87.22      84.22\n",
      "     86.22    ]\n",
      "   [ 31.220001  26.220001  28.220001 ...  84.22      85.22\n",
      "     86.22    ]\n",
      "   [ 28.220001  27.220001  28.220001 ...  85.22      86.22\n",
      "     87.22    ]]\n",
      "\n",
      "  [[ 78.06      79.06      78.06     ...  93.06      93.06\n",
      "     94.06    ]\n",
      "   [ 80.06      78.06      80.06     ...  93.06      93.06\n",
      "     94.06    ]\n",
      "   [ 79.06      81.06      79.06     ...  93.06      92.06\n",
      "     93.06    ]\n",
      "   ...\n",
      "   [ 42.059998  37.059998  40.059998 ... 100.06      97.06\n",
      "     99.06    ]\n",
      "   [ 44.059998  39.059998  41.059998 ...  97.06      98.06\n",
      "     99.06    ]\n",
      "   [ 41.059998  40.059998  41.059998 ...  98.06      99.06\n",
      "    100.06    ]]]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "keypoints=blob\n",
    "print(keypoints)\n",
    "img = image.copy()\n",
    "print(len(keypoints))\n",
    "#for x in range(0,len(keypoints)):\n",
    " # img2=cv2.circle(img, (np.int(keypoints[x].pt[0]),np.int(keypoints[x].pt[1])), radius=np.int(keypoints[x].size), color=(0,255,0), thickness=1000)\n",
    "#cv2.imshow(\"Image\",img2)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "net.setInput(blob)\n",
    "(scores, geometry) = net.forward(layerNames)\n",
    "#end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.object_detection import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11  72  37  87]\n",
      " [103  79 146  93]\n",
      " [110   9 178  22]\n",
      " [121 128 193 142]\n",
      " [111 142 203 158]\n",
      " [ 52 249  96 262]\n",
      " [112 230 199 245]\n",
      " [ 53  16  79  32]\n",
      " [161  88 187 102]\n",
      " [108  64 172  77]\n",
      " [ 56 162  86 175]\n",
      " [100  41 195  56]\n",
      " [105  23 199  38]\n",
      " [ 13 176  37 191]\n",
      " [129 104 158 118]\n",
      " [ 48 136  71 150]\n",
      " [104 161 146 173]\n",
      " [104 118 144 132]\n",
      " [ 14 253  42 264]\n",
      " [ 42 226  71 239]\n",
      " [113 217 192 232]\n",
      " [ 49 120  82 135]\n",
      " [126  91 156 105]\n",
      " [ 60  28  94  44]\n",
      " [ 13  18  37  31]\n",
      " [118 167 192 182]\n",
      " [ 14 201  37 214]\n",
      " [156  76 187  90]\n",
      " [139 244 189 257]\n",
      " [ 52 236  96 250]\n",
      " [ 49  55  69  72]\n",
      " [ 12 238  43 251]\n",
      " [ 14  56  43  70]\n",
      " [129 116 181 130]\n",
      " [253   1 272  15]\n",
      " [ 44  95  65 112]\n",
      " [ 43  81  61 100]\n",
      " [ 64 289  87 302]\n",
      " [103 284 152 298]\n",
      " [  6  31  33  47]\n",
      " [ 41 175  72 190]\n",
      " [255  15 269  33]\n",
      " [256  55 268  74]\n",
      " [232 278 248 302]\n",
      " [139 295 189 310]\n",
      " [ 13 189  36 202]\n",
      " [257 274 271 299]\n",
      " [157 282 194 295]\n",
      " [102 273 141 287]\n",
      " [254 102 268 127]\n",
      " [ 42 109  69 125]\n",
      " [105 259 149 273]\n",
      " [ 31   2  61  17]\n",
      " [ 50  42  71  57]\n",
      " [255 132 270 152]\n",
      " [ 11  44  40  59]\n",
      " [157 100 189 115]\n",
      " [110 154 192 170]\n",
      " [256  39 269  58]\n",
      " [ 12 225  40 238]\n",
      " [  9 164  36 180]\n",
      " [ 40 188  75 201]\n",
      " [153 256 186 268]\n",
      " [ 41  70  66  84]\n",
      " [256 218 270 242]\n",
      " [ 13 213  41 227]\n",
      " [102  93 135 106]\n",
      " [ 13 265  41 278]\n",
      " [256 182 268 205]\n",
      " [103  53 185  66]\n",
      " [104 105 134 118]\n",
      " [113 182 192 196]\n",
      " [253 160 269 178]\n",
      " [114 204 184 219]\n",
      " [  7 289  37 308]\n",
      " [ 15  85  39  98]\n",
      " [ 53 262  92 276]\n",
      " [ 52 148  74 162]\n",
      " [255 195 270 224]\n",
      " [ 76 147 101 161]\n",
      " [239 295 270 341]\n",
      " [ 97  -3 171  11]\n",
      " [256  78 268 101]\n",
      " [ 42 214  66 225]\n",
      " [102 196 134 210]\n",
      " [ 22  97  40 112]\n",
      " [254 148 271 165]\n",
      " [ 31 136  53 150]\n",
      " [164 270 189 282]\n",
      " [ 22 125  43 138]\n",
      " [253 237 273 252]\n",
      " [ 35 150  57 167]]\n"
     ]
    }
   ],
   "source": [
    "(rects, confidences) = decode_predictions(scores, geometry)\n",
    "#print(confidences)\n",
    "boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (startX, startY, endX, endY) in boxes:\n",
    "         # scale the bounding box coordinates based on the respective\n",
    "         # ratios\n",
    "         \n",
    "         #dX = int((endX - startX))*0.8\n",
    "         #dY = int((endY - startY))*0.8\n",
    "         startX = int(startX * rW)\n",
    "         startY = int(startY * rH)\n",
    "         endX = int(endX * rW)\n",
    "         endY = int(endY * rH)\n",
    " \n",
    "         # draw the bounding box on the image\n",
    "         \n",
    "         #dX = int((endX - startX))*0.8\n",
    "         #dY = int((endY - startY))*0.8\n",
    "         \n",
    " \n",
    "         # apply padding to each side of the bounding box, respectively\n",
    "        # startX = max(0, startX - dX)\n",
    "        # startY = max(0, startY - dY)\n",
    "        # endX = min(origW, endX + (dX * 2))\n",
    "        # endY = min(origH, endY + (dY * 2))\n",
    "        # print(type(startX),startY)\n",
    "         cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2) \n",
    "        # extract the actual padded ROI\n",
    "         #roi = orig[startY:endY, startX:endX]\n",
    " \n",
    "# show the output image\n",
    "cv2.imshow(\"Text Detection\", orig)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
